---Intro
The Hedgehog Open Network Fabric is an open networking platform that brings the user experience enjoyed by so many in the public cloud to private environments. It comes without vendor lock-in.

The Fabric is built around the concept of VPCs (Virtual Private Clouds), similar to public cloud offerings. It provides a multi-tenant API to define the user intent on network isolation and connectivity, which is automatically transformed into configuration for switches and software appliances.

You can read more about its concepts and architecture in the documentation.

You can find out how to download and try the Fabric on the self-hosted fully virtualized lab or on hardware.

---Concepts

Introduction
Hedgehog Open Network Fabric is built on top of Kubernetes and uses Kubernetes API to manage its resources. It means that all user-facing APIs are Kubernetes Custom Resources (CRDs), so you can use standard Kubernetes tools to manage Fabric resources.

Hedgehog Fabric consists of the following components:

Fabricator - special tool to install and configure Fabric, or to run virtual labs
Control Node - one or more Kubernetes nodes in a single cluster running Fabric software:
Fabric Controller - main control plane component that manages Fabric resources
Fabric Kubectl plugin (Fabric CLI) - kubectl plugin to manage Fabric resources in an easy way
Fabric Agent - runs on every switch and manages switch configuration
Fabric API
All infrastructure is represented as a set of Fabric resource (Kubernetes CRDs) and named Wiring Diagram. With this representation, Fabric defines switches, servers, control nodes, external systems and connections between them in a single place and then uses these definitions to deploy and manage the whole infrastructure. On top of the Wiring Diagram, Fabric provides a set of APIs to manage the VPCs and the connections between them and between VPCs and External systems.

Wiring Diagram API
Wiring Diagram consists of the following resources:

"Devices": describe any device in the Fabric and can be of two types:
Switch: configuration of the switch, containing for example: port group speeds, port breakouts, switch IP/ASN
Server: any physical server attached to the Fabric including Control Nodes
Connection: any logical connection for devices
usually it's a connection between two or more ports on two different devices
for example: MCLAG Peer Link, Unbundled/MCLAG server connections, Fabric connection between spine and leaf
VLANNamespace -> non-overlapping VLAN ranges for attaching servers
IPv4Namespace -> non-overlapping IPv4 ranges for VPC subnets
User-facing API
VPC API
VPC: Virtual Private Cloud, similar to a public cloud VPC, provides an isolated private network for the resources, with support for multiple subnets, each with user-defined VLANs and optional DHCP service
VPCAttachment: represents a specific VPC subnet assignment to the Connection object which means exact server port to a VPC binding
VPCPeering: enables VPC-to-VPC connectivity (could be Local where VPCs are used or Remote peering on the border/mixed leaves)
External API
External: definition of the "external system" to peer with (could be one or multiple devices such as edge/provider routers)
ExternalAttachment: configuration for a specific switch (using Connection object) describing how it connects to an external system
ExternalPeering: provides VPC with External connectivity by exposing specific VPC subnets to the external system and allowing inbound routes from it
Fabricator
Installer builder and VLAB.

Installer builder based on a preset (currently: vlab for virtual and lab for physical)
Main input: Wiring Diagram
All input artifacts coming from OCI registry
Always full airgap (everything running from private registry)
Flatcar Linux for Control Node, generated ignition.json
Automatic K3s installation and private registry setup
All components and their dependencies running in Kubernetes
Integrated Virtual Lab (VLAB) management
Future:
In-cluster (control) Operator to manage all components
Upgrades handling for everything starting Control Node OS
Installation progress, status and retries
Disaster recovery and backups
Fabric
Control plane and switch agent.

Currently Fabric is basically a single controller manager running in Kubernetes
It includes controllers for different CRDs and needs
For example, auto assigning VNIs to VPCs or generating the Agent configuration
Additionally, it's running the admission webhook for Hedgehog's CRD APIs
The Agent is watching for the corresponding Agent CRD in Kubernetes API
It applies the changes and saves the new configuration locally
It reports status and information back to the API
It can perform reinstallation and reboot of SONiC

---Install
Install Fabric
Prerequisites
A machine with access to the Internet to use Fabricator and build installer with at least 8 GB RAM and 25 GB of disk space
An 16 GB USB flash drive, if you are not using virtual media
Have a machine to function as the Fabric Control Node. System Requirements as well as IPMI access to it to install the OS.
A management switch with at least 1 10GbE port is recommended
Enough Supported Switches for your Fabric
Overview of Install Process
This section is dedicated to the Hedgehog Fabric installation on bare-metal control node(s) and switches, their preparation and configuration. To install the VLAB see VLAB Overview.

Download and install hhfab following instructions from the Download section.

The main steps to install Fabric are:

Install hhfab on the machines with access to the Internet
Prepare Wiring Diagram
Select Fabric Configuration
Build Control Node configuration and installer
Install Control Node
Insert USB with control-os image into Fabric Control Node
Boot the node off the USB to initiate the installation
Prepare Management Network
Connect management switch to Fabric control node
Connect 1GbE Management port of switches to management switch
Prepare supported switches
Ensure switch serial numbers and / or first management interface MAC addresses are recorded in wiring diagram
Boot them into ONIE Install Mode to have them automatically provisioned
Build Control Node configuration and Installer
Hedgehog has created a command line utility, called hhfab, that helps generate the wiring diagram and fabric configuration, validate the supplied configurations, and generate an installation image (.img or .iso) suitable for writing to a USB flash drive or mounting via IPMI virtual media. The first hhfab command to run is hhfab init. This will generate the main configuration file, fab.yaml. fab.yaml is responsible for almost every configuration of the fabric with the exception of the wiring. Each command and subcommand have usage messages, simply supply the -h flag to your command or sub command to see the available options. For example hhfab vlab -h and hhfab vlab gen -h.

HHFAB commands to make a bootable image
hhfab init --wiring wiring-lab.yaml
The init command generates a fab.yaml file, edit the fab.yaml file for your needs
ensure the correct boot disk (e.g. /dev/sda) and control node NIC names are supplied
hhfab validate
hhfab build --mode iso
An ISO is best suited to use with IPMI based virtual media. If desired an IMG file suitable for writing to a USB drive, can be created by passing the --mode usb option. ISO is the default.
The installer for the fabric is generated in $CWD/result/. This installation image is named control-1-install-usb.iso and is 7.5 GB in size. Once the image is created, you can write it to a USB drive, or mount it via virtual media.

Write USB Image to Disk
This will erase data on the USB disk.


Linux
macOS
Insert the USB to your machine
Identify the path to your USB stick, for example: /dev/sdc
Issue the command to write the image to the USB drive
sudo dd if=control-1-install-usb.img of=/dev/sdc bs=4k status=progress

There are utilities that assist this process such as etcher.

Install Control Node
This control node should be given a static IP address. Either a lease or statically assigned.

Configure the server to use UEFI boot without secure boot

Attach the image to the server either by inserting via USB, or attaching via virtual media

Select boot off of the attached media, the installation process is automated

Once the control node has booted, it logs in automatically and begins the installation process

Optionally use journalctl -f -u flatcar-install.service to monitor progress
Once the installation is complete, the system automatically reboots.

After the system has shutdown but before the boot up process reaches the operating system, remove the USB image from the system. Removal during the UEFI boot screen is acceptable.

Upon booting into the freshly installed system, the fabric installation will automatically begin

If the insecure --dev flag was passed to hhfab init the password for the core user is HHFab.Admin!, the switches have two users created admin and op. admin has administrator privileges and password HHFab.Admin!, whereas the op user is a read-only, non-sudo user with password HHFab.Op!.
Optionally this can be monitored with journalctl -f -u fabric-install.service
The install is complete when the log emits "Control Node installation complete". Additionally, the systemctl status will show inactive (dead) indicating that the executable has finished.

Configure Management Network
The control node is dual-homed: it connects to two different networks, which are called management and external, respectively, in the fab.yaml file. The management network is for controlling the switches that comprise the fabric. It can be a simple broadcast domain with layer 2 connectivity. The management network is not accessible to machines or devices not associated with the fabric; it is a private, exclusive network. The control node connects to the management network via a 10 GbE interface. It runs a DHCP server, as well as a small HTTP server.

The external network allows the user to access the control node via their local IT network. It provides SSH access to the host operating system on the control node.

Fabric Manages Switches
Now that the install has finished, you can start interacting with the Fabric using kubectl, kubectl fabric and k9s, all pre-installed as part of the Control Node installer.

At this stage, the fabric hands out DHCP addresses to the switches via the management network. Optionally, you can monitor this process by going through the following steps: - enter k9s at the command prompt - use the arrow keys to select the pod named fabric-boot - the logs of the pod will be displayed showing the DHCP lease process - use the switches screen of k9s to see the heartbeat column to verify the connection between switch and controller. - to see the switches type :switches (like a vim command) into k9s

---Wiring
Overview
A wiring diagram is a YAML file that is a digital representation of your network. You can find more YAML level details in the User Guide section switch features and port naming and the api. It's mandatory for all switches to reference a SwitchProfile in the spec.profile of the Switch object. Only port naming defined by switch profiles could be used in the wiring diagram, NOS (or any other) port names aren't supported.

In the meantime, to have a look at working wiring diagram for Hedgehog Fabric, run the sample generator that produces working wiring diagrams:


ubuntu@sl-dev:~$ hhfab sample -h

NAME:
   hhfab sample - generate sample wiring diagram

USAGE:
   hhfab sample command [command options]

COMMANDS:
   spine-leaf, sl      generate sample spine-leaf wiring diagram
   collapsed-core, cc  generate sample collapsed-core wiring diagram
   help, h             Shows a list of commands or help for one command

OPTIONS:
   --help, -h  show help
Or you can generate a wiring diagram for a VLAB environment with flags to customize number of switches, links, servers, etc.:


ubuntu@sl-dev:~$ hhfab vlab gen --help
NAME:
   hhfab vlab generate - generate VLAB wiring diagram

USAGE:
   hhfab vlab generate [command options]

OPTIONS:
   --bundled-servers value      number of bundled servers to generate for switches (only for one of the second switch in the redundancy group or orphan switch) (default: 1)
   --eslag-leaf-groups value    eslag leaf groups (comma separated list of number of ESLAG switches in each group, should be 2-4 per group, e.g. 2,4,2 for 3 groups with 2, 4 and 2 switches)
   --eslag-servers value        number of ESLAG servers to generate for ESLAG switches (default: 2)
   --fabric-links-count value   number of fabric links if fabric mode is spine-leaf (default: 0)
   --help, -h                   show help
   --mclag-leafs-count value    number of mclag leafs (should be even) (default: 0)
   --mclag-peer-links value     number of mclag peer links for each mclag leaf (default: 0)
   --mclag-servers value        number of MCLAG servers to generate for MCLAG switches (default: 2)
   --mclag-session-links value  number of mclag session links for each mclag leaf (default: 0)
   --no-switches                do not generate any switches (default: false)
   --orphan-leafs-count value   number of orphan leafs (default: 0)
   --spines-count value         number of spines if fabric mode is spine-leaf (default: 0)
   --unbundled-servers value    number of unbundled servers to generate for switches (only for one of the first switch in the redundancy group or orphan switch) (default: 1)
   --vpc-loopbacks value        number of vpc loopbacks for each switch (default: 0)
Sample Switch Configuration

apiVersion: wiring.githedgehog.com/v1beta1
kind: Switch
metadata:
  name: ds3000-02
spec:
  boot:
    serial: ABC123XYZ
  role: server-leaf
  description: leaf-2
  profile: celestica-ds3000
  portBreakouts:
    E1/1: 4x10G
    E1/2: 4x10G
    E1/17: 4x25G
    E1/18: 4x25G
    E1/32: 4x25G
  redundancy:
    group: mclag-1
    type: mclag
Design Discussion
This section is meant to help the reader understand how to assemble the primitives presented by the Fabric API into a functional fabric.

VPC
A VPC allows for isolation at layer 3. This is the main building block for users when creating their architecture. Hosts inside of a VPC belong to the same broadcast domain and can communicate with each other, if desired a single VPC can be configured with multiple broadcast domains. The hosts inside of a VPC will likely need to connect to other VPCs or the outside world. To communicate between two VPC a peering will need to be created. A VPC can be a logical separation of workloads. By separating these workloads additional controls are available. The logical separation doesn't have to be the traditional database, web, and compute layers it could be development teams who need isolation, it could be tenants inside of an office building, or any separation that allows for better control of the network. Once your VPCs are decided, the rest of the fabric will come together. With the VPCs decided traffic can be prioritized, security can be put into place, and the wiring can begin. The fabric allows for the VPC to span more than one switch, which provides great flexibility.

Connection
A connection represents the physical wires in your data center. They connect switches to other switches or switches to servers.

Server Connections
A server connection is a connection used to connect servers to the fabric. The fabric will configure the server-facing port according to the type of the connection (MLAG, Bundle, etc). The configuration of the actual server needs to be done by the server administrator. The server port names are not validated by the fabric and used as metadata to identify the connection. A server connection can be one of:

Unbundled - A single cable connecting switch to server.
Bundled - Two or more cables going to a single switch, a LAG or similar.
MCLAG - Two cables going to two different switches, also called dual homing. The switches will need a fabric link between them.
ESLAG - Two to four cables going to different switches, also called multi-homing. If four links are used there will need to be four switches connected to a single server with four NIC ports.


Fabric Connections
Fabric connections serve as connections between switches, they form the fabric of the network.

VPC Peering
VPCs need VPC Peerings to talk to each other. VPC Peerings come in two varieties: local and remote.

Local VPC Peering
When there is no dedicated border/peering switch available in the fabric we can use local VPC peering. This kind of peering tries sends traffic between the two VPC's on the switch where either of the VPC's has workloads attached. Due to limitation in the Sonic network operating system this kind of peering bandwidth is limited to the number of VPC loopbacks you have selected while initializing the fabric. Traffic between the VPCs will use the loopback interface, the bandwidth of this connection will be equal to the bandwidth of port used in the loopback.

The dotted line in the diagram shows the traffic flow for local peering. The traffic originates in VPC 2, travels to the switch, travels out the first loopback port, into the second loopback port, and finally out the port destined for VPC 1.
Remote VPC Peering
Remote Peering is used when you need a high bandwidth connection between the VPCs, you will dedicate a switch to the peering traffic. This is either done on the border leaf or on a switch where either of the VPC's are not present. This kind of peering allows peer traffic between different VPC's at line rate and is only limited by fabric bandwidth. Remote peering introduces a few additional hops in the traffic and may cause a small increase in latency.

The dotted line in the diagram shows the traffic flow for remote peering. The traffic could take a different path because of ECMP. It is important to note that Leaf 3 cannot have any servers from VPC 1 or VPC 2 on it, but it can have a different VPC attached to it.
VPC Loopback
A VPC loopback is a physical cable with both ends plugged into the same switch, suggested but not required to be the adjacent ports. This loopback allows two different VPCs to communicate with each other. This is due to a Broadcom limitation.

---Fabric_Config
Fabric Configuration
Overview
The fab.yaml file is the configuration file for the fabric. It supplies the configuration of the users, their credentials, logging, telemetry, and other non wiring related settings. The fab.yaml file is composed of multiple YAML documents inside of a single file. Per the YAML spec 3 hyphens (---) on a single line separate the end of one document from the beginning of the next. There are two YAML documents in the fab.yaml file. For more information about how to use hhfab init, run hhfab init --help.

Typical HHFAB workflows
HHFAB for VLAB
For a VLAB user, the typical workflow with hhfab is:

hhfab init --dev
hhfab vlab gen
hhfab vlab up
The above workflow will get a user up and running with a spine-leaf VLAB.

HHFAB for Physical Machines
It's possible to start from scratch:

hhfab init (see different flags to cusomize initial configuration)
Adjust the fab.yaml file to your needs
hhfab validate
hhfab build
Or import existing config and wiring files:

hhfab init -c fab.yaml -w wiring-file.yaml -w extra-wiring-file.yaml
hhfab validate
hhfab build
After the above workflow a user will have a .img file suitable for installing the control node, then bringing up the switches which comprise the fabric.

Fab.yaml
Configure control node and switch users
Configuring control node and switch users is done either passing --default-password-hash to hhfab init or editing the resulting fab.yaml file emitted by hhfab init. You can specify users to be configured on the control node(s) and switches in the following format:


spec:
    config:
      control:
        defaultUser: # user 'core' on all control nodes
          password: "hashhashhashhashhash" # password hash
          authorizedKeys:
            - "ssh-ed25519 SecREKeyJumblE"

        fabric:
          mode: spine-leaf # "spine-leaf" or "collapsed-core"

          defaultSwitchUsers:
            admin: # at least one user with name 'admin' and role 'admin'
              role: admin
              #password: "$5$8nAYPGcl4..." # password hash
              #authorizedKeys: # optional SSH authorized keys
              #  - "ssh-ed25519 AAAAC3Nza..."
            op: # optional read-only user
              role: operator
              #password: "$5$8nAYPGcl4..." # password hash
              #authorizedKeys: # optional SSH authorized keys
              #  - "ssh-ed25519 AAAAC3Nza..."
Control node(s) user is always named core.

The role of the user,operator is read-only access to sonic-cli command on the switches. In order to avoid conflicts, do not use the following usernames: operator,hhagent,netops.

NTP and DHCP
The control node uses public ntp servers from cloudflare and google by default. The control node runs a dhcp server on the management network. See the example file.

Control Node
The control node is the host that manages all the switches, runs k3s, and serves images. This is the YAML document configure the control node:


apiVersion: fabricator.githedgehog.com/v1beta1
kind: ControlNode
metadata:
  name: control-1
  namespace: fab
spec:
  bootstrap:
   disk: "/dev/sda" # disk to install OS on, e.g. "sda" or "nvme0n1"
  external:
    interface: enp2s0 # interface for external
    ip: dhcp # IP address for external interface
  management:
    interface: enp2s1 # interface for management

# Currently only one ControlNode is supported
The management interface is for the control node to manage the fabric switches, not end-user management of the control node. For end-user management of the control node specify the external interface name.
Forward switch metrics and logs
There is an option to enable Grafana Alloy on all switches to forward metrics and logs to the configured targets using Prometheus Remote-Write API and Loki API. If those APIs are available from Control Node(s), but not from the switches, it's possible to enable HTTP Proxy on Control Node(s) that will be used by Grafana Alloy running on the switches to access the configured targets. It could be done by passing --control-proxy=true to hhfab init.

Metrics includes port speeds, counters, errors, operational status, transceivers, fans, power supplies, temperature sensors, BGP neighbors, LLDP neighbors, and more. Logs include agent logs.

Configuring the exporters and targets is currently only possible by editing the fab.yaml configuration file. An example configuration is provided below:


spec:
  config:
      ...
      defaultAlloyConfig:
        agentScrapeIntervalSeconds: 120
        unixScrapeIntervalSeconds: 120
        unixExporterEnabled: true
        lokiTargets:
          grafana_cloud: # target name, multiple targets can be configured
              basicAuth: # optional
                  password: "<password>"
                  username: "<username>"
              labels: # labels to be added to all logs
                  env: env-1
              url: https://logs-prod-021.grafana.net/loki/api/v1/push
              useControlProxy: true # if the Loki API is not available from the switches directly, use the Control Node as a proxy
        prometheusTargets:
          grafana_cloud: # target name, multiple targets can be configured
              basicAuth: # optional
                  password: "<password>"
                  username: "<username>"
              labels: # labels to be added to all metrics
                  env: env-1
              sendIntervalSeconds: 120
              url: https://prometheus-prod-36-prod-us-west-0.grafana.net/api/prom/push
              useControlProxy: true # if the Loki API is not available from the switches directly, use the Control Node as a proxy
              unixExporterCollectors: # list of node-exporter collectors to enable, https://grafana.com/docs/alloy/latest/reference/components/prometheus.exporter.unix/#collectors-list
                  - cpu
                  - filesystem
                  - loadavg
                  - meminfo
              collectSyslogEnabled: true # collect /var/log/syslog on switches and forward to the lokiTargets
For additional options, see the AlloyConfig struct in Fabric repo.

Complete Example File

apiVersion: fabricator.githedgehog.com/v1beta1
kind: Fabricator
metadata:
  name: default
  namespace: fab
spec:
  config:
    control:
      tlsSAN: # IPs and DNS names to access API
        - "customer.site.io"

      ntpServers:
      - time.cloudflare.com
      - time1.google.com

      defaultUser: # user 'core' on all control nodes
        password: "hash..." # password hash
        authorizedKeys:
          - "ssh-ed25519 hash..."

    fabric:
      mode: spine-leaf # "spine-leaf" or "collapsed-core"
      includeONIE: true
      defaultSwitchUsers:
        admin: # at least one user with name 'admin' and role 'admin'
          role: admin
          password: "hash..." # password hash
          authorizedKeys:
            - "ssh-ed25519 hash..."
        op: # optional read-only user
          role: operator
          password: "hash..." # password hash
          authorizedKeys:
            - "ssh-ed25519 hash..."

      defaultAlloyConfig:
        agentScrapeIntervalSeconds: 120
        unixScrapeIntervalSeconds: 120
        unixExporterEnabled: true
        collectSyslogEnabled: true
        lokiTargets:
          lab:
            url: http://url.io:3100/loki/api/v1/push
            useControlProxy: true
            labels:
              descriptive: name
        prometheusTargets:
          lab:
            url: http://url.io:9100/api/v1/push
            useControlProxy: true
            labels:
              descriptive: name
            sendIntervalSeconds: 120

---
apiVersion: fabricator.githedgehog.com/v1beta1
kind: ControlNode
metadata:
  name: control-1
  namespace: fab
spec:
  bootstrap:
    disk: "/dev/sda" # disk to install OS on, e.g. "sda" or "nvme0n1"
  external:
    interface: eno2 # interface for external
    ip: dhcp # IP address for external interface
  management:
    interface: eno1

# Currently only one ControlNode is supported